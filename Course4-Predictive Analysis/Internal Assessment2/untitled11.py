# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E9OCGLwzMCI0iifXlYWWcUlfwpIiz4Vn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("/content/winequality-red.csv")
data.head()

pd.options.display.max_columns = None
data.head()

data.shape
print("Number of columns: {}".format(data.shape[1]))
print("Number of rows: {}".format(data.shape[0]))
data.info()

data.isnull().sum()

df=data.dropna()

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
for column in df.select_dtypes(include=['float64','int64']):
  df[column] = sc.fit_transform(df[[column]])

df.head()

# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# for column in df.select_dtypes(include=['float64','int64']):
#   df[column] = sc.fit_transform(df[[column]])

# df.head()

def remove_outliers(df):
    df_cleaned = df.copy()  # Make a copy of the DataFrame to avoid modifying the original DataFrame

    for ft in df_cleaned.columns:
        if df_cleaned[ft].dtype == 'float64':  # Check if the column contains float values
            q1 = df_cleaned[ft].quantile(0.25)
            q3 = df_cleaned[ft].quantile(0.75)
            iqr = q3 - q1

            lower_bound = q1 - (1.5 * iqr)
            upper_bound = q3 + (1.5 * iqr)

            # Replace outliers with the nearest bound
            df_cleaned[ft] = np.where(df_cleaned[ft] > upper_bound, upper_bound,
                                       np.where(df_cleaned[ft] < lower_bound, lower_bound, df_cleaned[ft]))

            # Drop outliers from the DataFrame
            df_cleaned = df_cleaned[(df_cleaned[ft] >= lower_bound) & (df_cleaned[ft] <= upper_bound)]

    return df_cleaned
df = remove_outliers(df)

df=df.astype(int)

df.info()

corr = df.corr()
plt.figure(figsize=(20,12))
sns.heatmap(corr,annot=True,cmap='Greens')
plt.show()

X = df.drop(columns=['quality'])
y = df['quality']
X_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state =42)

classifier = {
    "Random Forest Classifier": RandomForestClassifier(),
    "K-Nearest Neighbors Classifier": KNeighborsClassifier(),
    "Voting Classifier": VotingClassifier(estimators=[
        ('rf', RandomForestClassifier()),
        ('knn', KNeighborsClassifier())
    ], voting='hard')
}

for name, clf in classifier.items():
    print(f"\n=========={name}===========")
    clf.fit(X_train, y_train)
    y_pred = clf.predict(x_test)

    # Evaluation Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"\n Accuracy: {accuracy}")
    print(f" Precision: {precision}")
    print(f" Recall: {recall}")
    print(f" F1 Score: {f1}")

    # Confusion Matrix
    print("\n Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    # Classification Report
    print("\n Classification Report:")
    print(classification_report(y_test, y_pred))